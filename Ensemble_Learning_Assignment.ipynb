{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZvXHzXjgXEL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                       Ensemble Learning Assignment\n",
        "\n",
        "1.Can we use Bagging for regression problems.\n",
        "\n",
        ". Bagging (Bootstrap Aggregating) is an ensemble technique that trains multiple base models (like decision trees) on different random subsets of the training data (with replacement), and aggregates their outputs to improve performance.\n",
        "\n",
        "In classification, outputs are typically combined via voting.\n",
        "\n",
        "In regression, outputs are combined via averaging.\n",
        "\n",
        "2.What is the difference between multiple model training and single model training.\n",
        "\n",
        ". The difference between multiple model training and single model training lies in how many models are used to learn from the data and make predictions.\n",
        "\n",
        " Single Model Training\n",
        "\n",
        "What it is: Training one model (e.g., a decision tree, SVM, or logistic regression) on your dataset.\n",
        "\n",
        "Example: Fit one logistic regression model on training data and use it for prediction.\n",
        "Advantages:\n",
        "\n",
        "\n",
        "Simple and fast.\n",
        "\n",
        "Easy to interpret.\n",
        "\n",
        "Less computational cost.\n",
        "\n",
        "Disadvantages:\n",
        "May overfit or underfit.\n",
        "\n",
        "Less robust to noise or data variance.\n",
        "\n",
        "Limited performance if the model is weak.\n",
        "\n",
        " Multiple Model Training (Ensemble Learning)\n",
        "\n",
        "What it is: Training multiple models and combining their outputs to make a final prediction.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Bagging (e.g., Random Forest)\n",
        "\n",
        "Boosting (e.g., XGBoost, AdaBoost)\n",
        "\n",
        "Stacking (combine different model types)\n",
        "\n",
        "Advantages:\n",
        "Often higher accuracy than a single model.\n",
        "\n",
        "Reduces overfitting and variance (especially in unstable models like decision trees).\n",
        "\n",
        "More robust and generalizable.\n",
        "\n",
        "Disadvantages:\n",
        "More complex and computationally expensive.\n",
        "\n",
        "Harder to interpret.\n",
        "\n",
        "Takes more time to train and tune.\n",
        "\n",
        "3.Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        ".Feature Randomness in Random Forest — Explained\n",
        "Feature randomness is a key technique used in Random Forest to make the model more robust, accurate, and less prone to overfitting.\n",
        "\n",
        "In a Decision Tree, each split is chosen by evaluating all features to find the best one (based on Gini impurity, entropy, etc.).\n",
        "\n",
        "In a Random Forest, instead of checking all features at every split, the algorithm randomly selects a subset of features to consider at each node.\n",
        "\n",
        "This random selection is called feature randomness.\n",
        "\n",
        "4.What is OOB (Out-of-Bag) Score.\n",
        "\n",
        ".The OOB (Out-of-Bag) score is a built-in cross-validation method used in Random Forests to estimate the model’s performance without needing a separate validation set.\n",
        "\n",
        "Random Forest uses bagging, which means:\n",
        "\n",
        "Each tree is trained on a bootstrap sample: a random sample with replacement from the training data.\n",
        "\n",
        "As a result, about 63% of the data points are used in training each tree.\n",
        "\n",
        "The remaining ~37% of the data — which was not selected — is called Out-of-Bag samples.\n",
        "\n",
        "5.How can you measure the importance of features in a Random Forest model.\n",
        "\n",
        ". Random Forests not only make accurate predictions but also provide a built-in mechanism to measure feature importance — i.e., how much each feature contributes to the model's predictions.\n",
        "\n",
        "Mean Decrease in Impurity (MDI) – aka Gini Importance\n",
        "This is the default method used in most implementations.\n",
        "\n",
        "It measures how much each feature reduces impurity (e.g., Gini, entropy) across all trees in the forest.\n",
        "\n",
        "The more a feature is used to make important splits, the higher its importance score.\n",
        "\n",
        "6.Explain the working principle of a Bagging Classifier.\n",
        "\n",
        ". Bagging stands for Bootstrap Aggregating. It's an ensemble learning technique that builds a stronger model by combining multiple weaker models (usually the same type) trained on different subsets of the data.\n",
        "\n",
        "Bootstrapping the Data:\n",
        "\n",
        "From the original training data, create multiple random subsets (with replacement).\n",
        "\n",
        "Each subset may contain duplicate samples due to replacement.\n",
        "\n",
        "These are called bootstrap samples.\n",
        "\n",
        "7.How do you evaluate a Bagging Classifier’s performance.\n",
        "\n",
        ". Evaluating a Bagging Classifier is similar to evaluating any classification model, but with added considerations for ensemble behavior. You typically use standard classification metrics and optionally Out-of-Bag (OOB) estimates\n",
        "\n",
        "Train/Test Split or Cross-Validation\n",
        "Use a holdout set or k-fold cross-validation to assess how well the model generalizes.\n",
        "\n",
        "Common Metrics:\n",
        "\n",
        "Accuracy  % of correct predictions.\n",
        "\n",
        "Confusion Matrix detailed breakdown of correct/incorrect by class.\n",
        "\n",
        "Precision, Recall, F1-Score especially important for imbalanced datasets.\n",
        "\n",
        "ROC-AUC  good for binary classification\n",
        "\n",
        "8.How does a Bagging Regressor work.\n",
        "\n",
        ".A Bagging Regressor is the regression counterpart of a Bagging Classifier. It uses ensemble learning to improve the performance of regression models by combining predictions from multiple base estimators trained on different subsets of the training data.\n",
        "\n",
        "9.What is the main advantage of ensemble techniques.\n",
        "\n",
        ". The main advantage of ensemble techniques is:\n",
        "\n",
        " Improved predictive performance — in terms of accuracy, robustness, and generalization — compared to a single model.\n",
        "\n",
        " Ensemble methods combine multiple models (often called base learners or weak learners) to form a stronger, more reliable predictor.\n",
        "\n",
        "They reduce:\n",
        "\n",
        "Variance (e.g., Bagging like Random Forest)\n",
        "\n",
        "Bias (e.g., Boosting like XGBoost)\n",
        "\n",
        "Overfitting (by averaging or voting)\n",
        "\n",
        "10.What is the main challenge of ensemble methods.\n",
        "\n",
        ". The main challenge of ensemble methods is:\n",
        "\n",
        " Increased complexity and computational cost — both in terms of training time, model interpretability, and resource usage.\n",
        "\n",
        " 11.Explain the key idea behind ensemble techniques.\n",
        "\n",
        " . Key Idea Behind Ensemble Techniques\n",
        "The key idea behind ensemble techniques is:\n",
        "\n",
        "Combine multiple individual models to create a stronger, more accurate, and more robust predictive model than any single model alone.\n",
        "\n",
        "Diversity: Base models should be diverse (different algorithms, different data subsets, or different parameter settings).\n",
        "\n",
        "Combination: Predictions from base models are combined using methods like:\n",
        "\n",
        "Majority voting (for classification)\n",
        "\n",
        "Averaging (for regression)\n",
        "\n",
        "Meta-modeling (stacking)\n",
        "\n",
        "12.What is a Random Forest Classifier?\n",
        "\n",
        ". A Random Forest Classifier is an ensemble learning method used for classification tasks that builds multiple decision trees during training and outputs the class that is the majority vote of the individual trees.\n",
        "\n",
        "13.What are the main types of ensemble techniques.\n",
        "\n",
        ". There are three primary types of ensemble methods in machine learning:\n",
        "\n",
        "Bagging (Bootstrap Aggregating)\n",
        "Goal: Reduce variance and prevent overfitting.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Create multiple random subsets of training data with replacement (bootstrapping).\n",
        "\n",
        "Train a base model (often decision trees) on each subset independently.\n",
        "\n",
        "Aggregate predictions by majority voting (classification) or averaging (regression).\n",
        "\n",
        "Boosting\n",
        "Goal: Reduce bias and improve weak learners by focusing on errors.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Models are trained sequentially.\n",
        "\n",
        "Each new model focuses on correcting errors made by previous models.\n",
        "\n",
        "Final prediction is a weighted combination of all models.\n",
        "\n",
        "14.What is ensemble learning in machine learning?\n",
        "\n",
        ". Ensemble learning is a technique in machine learning where multiple models (often called base learners) are trained and combined to solve a problem — typically leading to better performance than any individual model alone.\n",
        "\n",
        "15.When should we avoid using ensemble methods?\n",
        "\n",
        ". While ensemble methods can boost model performance, they’re not always the best choice. Here are situations where you should consider avoiding them:\n",
        "\n",
        "When Interpretability Is Crucial\n",
        "Ensembles (especially Random Forests, Gradient Boosting, etc.) are black-box models.\n",
        "\n",
        "If you need to explain decisions (e.g., in healthcare or finance), a simpler model like logistic regression or a single decision tree may be more appropriate.\n",
        "\n",
        "16.How does Bagging help in reducing overfitting?\n",
        "\n",
        ". Bagging (Bootstrap Aggregating) helps reduce overfitting by:\n",
        "\n",
        "Averaging multiple models trained on different random subsets of the data, which lowers variance and improves generalization.\n",
        "\n",
        "The Intuition:\n",
        "A single model (e.g., a decision tree) can easily overfit — especially if it's very deep and complex.\n",
        "\n",
        "But many such models, trained on different bootstrapped samples of the data, will each overfit in slightly different ways.\n",
        "\n",
        "When their predictions are averaged (for regression) or voted on (for classification), the errors tend to cancel out, reducing overall overfitting.\n",
        "\n",
        "17.Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        ". Why Random Forest Performs Better:\n",
        "Reduces Overfitting\n",
        "\n",
        "A single decision tree may memorize the training data.\n",
        "\n",
        "A random forest averages many diverse trees, reducing variance.\n",
        "\n",
        "Handles Feature Correlation\n",
        "\n",
        "Each tree considers only a subset of features, which encourages feature diversity and reduces reliance on dominant features.\n",
        "\n",
        "Better Generalization\n",
        "\n",
        "Ensemble of trees gives better out-of-sample performance than one tree.\n",
        "\n",
        "Robust to Noise and Outliers\n",
        "\n",
        "18.What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        ". Bootstrap sampling is a core component of the Bagging (Bootstrap Aggregating) technique. It plays a key role in creating diverse models by generating multiple, slightly different datasets from the original training data.\n",
        "\n",
        " What Is Bootstrap Sampling?\n",
        "\n",
        "It means sampling with replacement from the original dataset.\n",
        "\n",
        "Each new dataset (bootstrap sample) has the same size as the original, but may contain duplicate records.\n",
        "\n",
        "19.What are some real-world applications of ensemble techniques?\n",
        "\n",
        ". Real-World Applications of Ensemble Techniques (Short Answer)\n",
        "Finance – Fraud detection, credit scoring\n",
        "\n",
        "Healthcare – Disease prediction, medical diagnosis\n",
        "\n",
        "Marketing – Customer segmentation, churn prediction\n",
        "\n",
        "E-commerce – Product recommendation, price prediction\n",
        "\n",
        "Cybersecurity – Intrusion detection, malware classification\n",
        "\n",
        "Image Recognition – Facial recognition, object detection\n",
        "\n",
        "Natural Language Processing – Sentiment analysis, spam filtering\n",
        "\n",
        "Insurance – Risk assessment, claim prediction\n",
        "\n",
        "Weather Forecasting – Ensemble climate models\n",
        "\n",
        "Search Engines – Ranking algorithms and relevance prediction\n",
        "\n",
        "20.What is the difference between Bagging and Boosting?\n",
        "\n",
        ". Bagging vs. Boosting (Short Answer)\n",
        "Bagging builds multiple independent models in parallel using random subsets of data to reduce variance (e.g., Random Forest).\n",
        "\n",
        "Boosting builds models sequentially, where each new model corrects the errors of the previous ones to reduce bias and variance (e.g., AdaBoost, Gradient Boosting).\n",
        "\n",
        "Think:\n",
        "\n",
        "Bagging = “Parallel teamwork”\n",
        "\n",
        "Boosting = “Step-by-step improvement”\n",
        "\n",
        "Let me know if you'd like a quick visual or code demo!\n",
        "\n",
        "                       Practical\n",
        "\n",
        "21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VnaOBzpvXF1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Define Bagging Classifier\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=base_tree,\n",
        "    n_estimators=50,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1TYDV5lNuCSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n"
      ],
      "metadata": {
        "id": "6OsFFhc0uGSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a regression dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_tree = DecisionTreeRegressor()\n",
        "\n",
        "# Define Bagging Regressor\n",
        "bagging_model = BaggingRegressor(\n",
        "    base_estimator=base_tree,\n",
        "    n_estimators=50,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate and print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "SA2S7iU1uPFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n"
      ],
      "metadata": {
        "id": "euGoWcWOuPrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "jKDSa-izuSvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a Random Forest Regressor and compare its performance with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "JJrhVKH-ugEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "tree = DecisionTreeRegressor(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "y_pred_forest = forest.predict(X_test)\n",
        "mse_forest = mean_squared_error(y_test, y_pred_forest)\n",
        "\n",
        "# Print results\n",
        "print(f\"Decision Tree MSE: {mse_tree:.2f}\")\n",
        "print(f\"Random Forest MSE: {mse_forest:.2f}\")\n"
      ],
      "metadata": {
        "id": "SsOIauwyulpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n"
      ],
      "metadata": {
        "id": "rCVRGeQGvZVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data (optional, OOB works on training set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest with OOB enabled\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"Out-of-Bag Score: {rf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "RnOil8HYvfGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Bagging Classifier using SVM as a base estimator and print accuracy\n"
      ],
      "metadata": {
        "id": "RZbtO5BFvidt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator (SVM with probability=True for better Bagging compatibility)\n",
        "base_svm = SVC(probability=True, kernel='rbf', random_state=42)\n",
        "\n",
        "# Define Bagging Classifier with SVM base estimator\n",
        "bagging_svm = BaggingClassifier(\n",
        "    base_estimator=base_svm,\n",
        "    n_estimators=10,\n",
        "    random_state=42,\n",
        "    bootstrap=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "_AyZ9tqPwA8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a Random Forest Classifier with different numbers of trees and compare accuracy\n"
      ],
      "metadata": {
        "id": "K2Yzas_Vz0KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of trees to try\n",
        "n_trees = [1, 5, 10, 50, 100, 200]\n",
        "\n",
        "# Store results\n",
        "accuracies = []\n",
        "\n",
        "for n in n_trees:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Random Forest with {n} trees — Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "O2NlebWU0Csj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n"
      ],
      "metadata": {
        "id": "eGEpOYL80DE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimator\n",
        "base_lr = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "\n",
        "# Define Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=base_lr,\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    bootstrap=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_probs = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC: {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "nE9LLfm00L23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train a Random Forest Regressor and analyze feature importance scores\n"
      ],
      "metadata": {
        "id": "bpCjMHXB0MPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "feature_names = load_diabetes().feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feature_names, importances, color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance - Random Forest Regressor')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dvkCqAMl0gZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n"
      ],
      "metadata": {
        "id": "Xbs72Cce0gxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    bootstrap=True\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "areD3us402Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n"
      ],
      "metadata": {
        "id": "PDjmeK_N023v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Train with hyperparameter tuning\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score from GridSearchCV\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set using best estimator\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "r1-Cs2vy09pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.Train a Bagging Regressor with different numbers of base estimators and compare performance\n"
      ],
      "metadata": {
        "id": "Wa7ZMttG1S4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of base estimators\n",
        "n_estimators_list = [1, 5, 10, 50, 100]\n",
        "\n",
        "# Store MSE results\n",
        "mse_results = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42,\n",
        "        bootstrap=True\n",
        "    )\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_results.append(mse)\n",
        "    print(f\"Bagging Regressor with {n} estimators - MSE: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "HYhM_dD01trK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33.Train a Random Forest Classifier and analyze misclassified samples.\n"
      ],
      "metadata": {
        "id": "T2vvT_qI1uNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "feature_names = load_breast_cancer().feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "\n",
        "# Create DataFrame with misclassified samples details\n",
        "df_misclassified = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "df_misclassified['True Label'] = y_test[misclassified_indices]\n",
        "df_misclassified['Predicted Label'] = y_pred[misclassified_indices]\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(df_misclassified)\n",
        "\n",
        "# Optional: Show the first few misclassified samples only\n",
        "#print(df_misclassified.head())\n"
      ],
      "metadata": {
        "id": "R2pewnxL11KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n"
      ],
      "metadata": {
        "id": "BhO2FKyq14Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    bootstrap=True\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print results\n",
        "print(f\"Decision Tree Accuracy: {acc_dt:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {acc_bagging:.4f}\")\n"
      ],
      "metadata": {
        "id": "XvxgL0Xp17h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a Random Forest Classifier and visualize the confusion matrix\n"
      ],
      "metadata": {
        "id": "YolDJnxC1762"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Random Forest Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tvbQh1-F2B5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n"
      ],
      "metadata": {
        "id": "AVxWjmlM3ATT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Define Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "FUtrcJx43DTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.Train a Random Forest Classifier and print the top 5 most important features\n"
      ],
      "metadata": {
        "id": "2-1I52yw3GGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for feature importances\n",
        "feat_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feat_df.head(5))\n"
      ],
      "metadata": {
        "id": "Rq1RmN8V3MS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "agq5_mOw3M32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate using Precision, Recall, F1-score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=load_iris().target_names))\n"
      ],
      "metadata": {
        "id": "GxIdKQCi3ThW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n"
      ],
      "metadata": {
        "id": "mn5vCCe33T42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Track results\n",
        "depths = list(range(1, 21))\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate models with different max_depth values\n",
        "for depth in depths:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(depths, accuracies, marker='o', linestyle='-', color='blue')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(depths)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k9jXU0m23rcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "l9lMnKp83sIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree Regressor\n",
        "bagging_dt = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Bagging with KNeighbors Regressor\n",
        "bagging_knn = BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=100, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Compare Results\n",
        "print(f\"Bagging Regressor with Decision Tree MSE: {mse_dt:.4f}\")\n",
        "print(f\"Bagging Regressor with KNeighbors MSE:    {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "I668zKGL3_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n"
      ],
      "metadata": {
        "id": "v-J4SFjl4AZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n8_5KQYK4HV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n"
      ],
      "metadata": {
        "id": "QUopMhyv4HuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Tree\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Evaluate using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy: {:.4f}\".format(np.mean(cv_scores)))\n",
        "print(\"Standard Deviation: {:.4f}\".format(np.std(cv_scores)))\n"
      ],
      "metadata": {
        "id": "wz_6O81D4ZDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.Train a Random Forest Classifier and plot the Precision-Recall curv\n"
      ],
      "metadata": {
        "id": "Q_HgfvCR4ZcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Precision-Recall values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, color='purple', label=f'AP = {avg_precision:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Random Forest')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Olww5PXE4f65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n"
      ],
      "metadata": {
        "id": "0HvnqaLT4gVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Define stacking classifier\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=[('rf', rf), ('lr', lr)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True,  # optional: allows final estimator to access original features\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Fit and evaluate Random Forest\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Fit and evaluate Logistic Regression\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "# Fit and evaluate Stacking Classifier\n",
        "stack_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stack_clf.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Random Forest Accuracy:         {acc_rf:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy:   {acc_lr:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy:   {acc_stack:.4f}\")\n"
      ],
      "metadata": {
        "id": "_VUaTsYM4mJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n"
      ],
      "metadata": {
        "id": "1Cgi8Clc4mi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different bootstrap sample sizes (as a fraction of the training set)\n",
        "sample_fractions = [0.3, 0.5, 0.7, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor with varying bootstrap sample sizes\n",
        "for frac in sample_fractions:\n",
        "    bagging = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=100,\n",
        "        max_samples=frac,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging.fit(X_train, y_train)\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"Bootstrap Sample Fraction {frac:.1f} -> MSE: {mse:.2f}\")\n",
        "\n",
        "# Optional: Plot the performance\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(sample_fractions, mse_scores, marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Bootstrap Sample Fraction')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('MSE vs. Bootstrap Sample Fraction (Bagging Regressor)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fazpfxpC4vRW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}